{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7faaed8",
   "metadata": {},
   "source": [
    "# 07: Neural Networks II\n",
    "Useful resources:\n",
    "- [Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers](https://arxiv.org/pdf/1801.06889.pdf)\n",
    "- [Distill](https://distill.pub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aeffc8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fb0a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pmlb\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ce258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're running this code locally, then you can uncomment this to automatically\n",
    "# save the chart data in files, rather than including the data in the spec. \n",
    "\n",
    "# !mkdir -p data\n",
    "# alt.data_transformers.enable('json', prefix='data/altair-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea058baa",
   "metadata": {},
   "source": [
    "## Data Preparation and Modeling\n",
    "\n",
    "Load the mnist dataset and take a random sample of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67891527",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = pmlb.fetch_data('mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1496f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_small = mnist.sample(n=30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5351cc64",
   "metadata": {},
   "source": [
    "Separate the feature values from the target labels. Split the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aea80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist_small.drop(columns=['target']).values\n",
    "y = mnist_small['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e749bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(list(set(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc9450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcdc483",
   "metadata": {},
   "source": [
    "Next we'll train a multi-layer perceptron on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42722396",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPClassifier(hidden_layer_sizes=(512, 256))\n",
    "nn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31140c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24119851",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db7a25",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e915455",
   "metadata": {},
   "source": [
    "This model has 4 layers: an input layer, two hidden layers, and an output layer. The hidden layers use the ReLU activation function. The output layer uses the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc281549",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.n_layers_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed31e1e1",
   "metadata": {},
   "source": [
    "The `get_layer_output` function below returns the output of the model at a given layer.\n",
    "\n",
    "References:\n",
    "- [sklearn source code for generating model predictions](https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b611bf873bd5836748647221480071a87/sklearn/neural_network/_multilayer_perceptron.py#L144).\n",
    "- [sklearn source code for ReLU and softmax activations](https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b611bf873bd5836748647221480071a87/sklearn/neural_network/_base.py#L47)\n",
    "- [scipy source code for softmax](https://github.com/scipy/scipy/blob/v1.9.3/scipy/special/_logsumexp.py#L130-L223)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08715df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return np.maximum(X, 0)\n",
    "\n",
    "def softmax(X):\n",
    "    return np.exp(X) / np.exp(X).sum(axis=1, keepdims=True)\n",
    "\n",
    "def get_layer_output(model, X, layer):\n",
    "    output = X\n",
    "    \n",
    "    for i in range(layer - 1):\n",
    "        z = np.dot(output, model.coefs_[i]) + model.intercepts_[i]\n",
    "        \n",
    "        if i < model.n_layers_ - 2:\n",
    "            output = relu(z)\n",
    "        else:\n",
    "            output = softmax(z)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c701cc",
   "metadata": {},
   "source": [
    "For example, we can see that getting the output of the last layer is the same as calling the model's `predict_proba` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b382f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.predict_proba(X_train[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d5f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_layer_output(nn, X_train[0:3], nn.n_layers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fd06ea",
   "metadata": {},
   "source": [
    "The `get_df` function below transforms a numpy array representing an image into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b9d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(data):\n",
    "    indices = np.arange(data.shape[0])\n",
    "    size = int(np.sqrt(data.shape[0]))\n",
    "    x = indices % size\n",
    "    y = np.floor(indices / size)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'x': x,\n",
    "        'y': y,\n",
    "        'value': data\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa60e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_df(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5571ab65",
   "metadata": {},
   "source": [
    "## Neuron Activation Matrix\n",
    "\n",
    "The [ActiVis paper](https://arxiv.org/pdf/1704.01942.pdf) contains a neuron activation matrix. Each row in the matrix represents a subset of instances. Each column represents a neuron in the neural network. Let's create a version of this matrix for our model. We will subset the instances by their true class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120a4217",
   "metadata": {},
   "source": [
    "**Exercise 1:**\n",
    "\n",
    "First, we need a function that computes the average activation for each hidden neuron in our model for a given set of instances. This function should return a flat list of the average activations for the neurons in the hidden layers. In our case, there are two hidden layers with a combined total of 768 neurons, so this list should contain 768 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d0093",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X - 2D numpy array representing a set of instances\n",
    "nn - sklearn MLPClassifier\n",
    "'''\n",
    "def get_average_activations(X, nn):\n",
    "    activations = []\n",
    "    \n",
    "    for layer in range(2, nn.n_layers_):\n",
    "        activations.extend(get_layer_output(nn, X, layer).mean(axis=0))\n",
    "\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ab3868",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_average_activations(X_test[y_test == 0], nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8136dc47",
   "metadata": {},
   "source": [
    "**Exercise 2:**\n",
    "\n",
    "Next, we need a function that puts the average activations for each subset of instances into a single dataframe. We will split the instances into subsets based on their ground-truth label. Each row of the dataframe will represent a pair of an instance subset and a neuron (i.e. each row is one cell in the matrix). The dataframe will have three columns:\n",
    "- \"neuron\" for the index of the neuron\n",
    "- \"label\" for the label of the instances in the subset\n",
    "- \"activation\" for the average activation of the neuron for the instances in the subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2d108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X - 2D numpy array containing the instances\n",
    "y - 1D numpy array containing the ground-truth labels\n",
    "nn - sklearn MLPClassifier\n",
    "'''\n",
    "def calculate_activation_matrix(X, y, nn):\n",
    "    dfs = []\n",
    "    \n",
    "    # 2a: get a list of the unique labels\n",
    "    classes = sorted(list(set(y)))\n",
    "    \n",
    "    # 2b: we'll refer to the neurons by their index in the lists returned\n",
    "    # by get_average_activations. for convenience, we'll get the list of\n",
    "    # neuron indices here (i.e. [0, 1, 2, 3, ..., 766, 767])\n",
    "    num_neurons = sum(nn.hidden_layer_sizes)\n",
    "    neuron_ids = list(range(num_neurons))\n",
    "    \n",
    "    for label in classes:\n",
    "        # 2c: get the average activation for the instances with this label\n",
    "        activations = get_average_activations(X[y == label], nn)\n",
    "        \n",
    "        # 2d: create a dataframe for this subset\n",
    "        df = pd.DataFrame({\n",
    "            'neuron': neuron_ids,\n",
    "            'label': label,\n",
    "            'activation': activations\n",
    "        })\n",
    "        \n",
    "        dfs.append(df)\n",
    "        \n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441dbd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = calculate_activation_matrix(X_test, y_test, nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c6b5bc",
   "metadata": {},
   "source": [
    "**Exercise 3:**\n",
    "\n",
    "768 neurons is too many to show at once. In the ActiVis paper, you can choose the neurons that have the highest activations for a given class. Complete the `get_top_neurons` function below. It should return a sorted list of the top `num_neurons` neurons that have the highest average activation for the instances with the given `label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c65ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "activations - pandas dataframe\n",
    "num_neurons - number of neurons to take\n",
    "label - subset of instances to sort the neurons by\n",
    "'''\n",
    "def get_top_neurons(activations, num_neurons, label):\n",
    "    # filter the df to only contain subsets with the given label\n",
    "    activations = activations[activations[\"label\"] == label]\n",
    "    mean_by_neuron = activations.groupby('neuron').mean()\n",
    "    top_neurons = mean_by_neuron.sort_values('activation', ascending=False).index[0:num_neurons]\n",
    "    return list(top_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d93f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dde6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_neurons_0 = get_top_neurons(activations, num_neurons, 0)\n",
    "top_neurons_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b8d26",
   "metadata": {},
   "source": [
    "**Exercise 4:**\n",
    "\n",
    "Once we've computed a sorted list of `neurons`, we need a function that will filter the `activations` dataframe to only contain those neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7493f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "activations - pandas dataframe\n",
    "neurons - list containing indices of neurons\n",
    "'''\n",
    "def filter_activations(activations, neurons):\n",
    "    return activations[activations['neuron'].isin(neurons)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88163712",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_activations(activations, top_neurons_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8468d74",
   "metadata": {},
   "source": [
    "**Exercise 5:**\n",
    "\n",
    "Complete the `neuron_activation_matrix` function below. It should return a heatmap of the activations for the top `num_neurons` for the given `label`. You should call `get_top_neurons` and `filter_activations` in this function.\n",
    "\n",
    "Note: the heatmap will still have one row per label. The `label` argument determines which subset of instances is used to sort the neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8496298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "activations - pandas dataframe\n",
    "num_neurons - number of neurons (columns) to show in the matrix\n",
    "label - subset of instances to sort the neurons by\n",
    "'''\n",
    "def neuron_activation_matrix(activations, num_neurons, label):\n",
    "    neurons = get_top_neurons(activations, num_neurons, label)\n",
    "    activations = filter_activations(activations, neurons)\n",
    "    return alt.Chart(activations).mark_rect().encode(\n",
    "        x=alt.X('neuron:N', sort=neurons),\n",
    "        y='label:N',\n",
    "        color='activation'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf086eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_activation_matrix(activations, num_neurons, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f2f480",
   "metadata": {},
   "source": [
    "**Exercise 6:**\n",
    "\n",
    "Concatenate the neuron activation matrices sorted by each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb6130",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "activations - pandas dataframe\n",
    "num_neurons - number of columns in each matrix\n",
    "labels - list of numbers\n",
    "'''\n",
    "def all_neuron_activation_matrices(activations, num_neurons, labels):\n",
    "    charts = [neuron_activation_matrix(activations, num_neurons, label) for label in labels]\n",
    "    return alt.vconcat(*charts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dcc5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_neuron_activation_matrices(activations, num_neurons, range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78052661",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
